\chapter{Conclusões e propostas de trabalhos futuros} \label{cap:conclusao}

Este trabalho teve como objetivo a realização de um estudo das estratégias de agrupamento de séries temporais para a posterior aplicação das técnicas estudadas no agrupamento de dados de curvas de consumo de cargas residenciais. Dessa maneira, inicialmente, as possíveis escolhas ao se realizar o agrupamento de séries temporais foram analisadas.

As técnicas de pré-processamento foram discutidas na qual foram expostas algumas técnicas de redução de dimensionalidade como a PAA ou TDF, e se verificou que algum tipo de normalização das curvas deve ser realizada para que a comparação entre as curvas faça sentido. Em seguida, foram expostas as medidas de dissimilaridade entre as curvas, no qual obtiveram destaque algumas medidas como a DTW e EDR, além da correção das dissimilaridades pela CID \parencite{CID}. Logo após, foram apresentados os principais algoritmos encontrados na literatura como os algoritmos \emph{k-means} e hierárquicos. Finalmente, os índices de validação de agrupamentos foram discutidos, dando-se certo destaque para o índice de validação externo de Variação da Informação (VI), pelo fato deste ser uma métrica no espaço de partições.

A partir da revisão bibliográfica realizada no Capítulo ~\ref{cap:agrupamento_series_temporais}, no Capítulo ~\ref{cap:testes_teoricos} realizou-se um estudo empírico de agrupamento de bases de dados rotuladas, onde os resultados foram comparados com o resultado esperado pelo índice VI. Nos testes foram realizados agrupamentos  , em cada base de dados, variando-se os principais algoritmos e medidas de dissimilaridade mencionados na literatura. Testes estatísticos de significância apontaram que os algoritmos hierárquicos, principalmente ao se utilizar a \emph{linkagem} \emph{average}, possuem performance melhor que os algoritmos particionais. 

Realizando-se os agrupamentos com o algoritmo hierárquico e \emph{linkagem average} e diferentes métricas de dissimilaridade, verificou-se que as dissimilaridades DTW \parencite{DTW} possuem desempenho superior. No entanto, ao se aplicar a CID à dissimilaridades como a euclidiana, estas apresentaram desempenho estatisticamente equivalente ao desempenho da DTW quando também corrigida pela CID. Este resultado é interessante já que existe grande diferença de custos computacionais para se obter a DTW e distância euclidiana entre duas séries temporais, e que este custo para a última é significativamente menor.

Em seguida, foram realizados testes para analisar a performance dos índices de validação interno. Em um primeiro experimento, os índices internos foram testados para indicar o número de grupos  existentes em uma partição e, neste experimento, após as análises de significância, chegou-se a conclusão que o índice Calinski-Harabazs \parencite{CH} é o mais adequado para a realização de tal tarefa no contexto de séries temporais. Uma vez definido o número de grupos, surge a pergunta de qual índice de validação interno é o mais adequado para avaliar as partições obtidas por diferentes abordagens no que diz respeito às escolhas da técnica de pré-processamento, algoritmo de agrupamento e métrica de dissimilaridade. Para responder à esta pergunta, novos testes foram realizados nas bases de dados rotuladas, e os resultados mostraram que os índices silhouette \parencite{silhouette} e Dunn ~\parencite{Dunn} possuem performance significativamente superior aos outros quatro índices testados.

Dessa maneira, partindo-se do pressuposto que não se possui, \emph{a priori}, nenhuma informação útil para a realização do agrupamento, como, por exemplo, o número de grupos que as instâncias se encontram naturalmente divididas, os experimentos realizados em bases de dados rotuladas sugerem a seguinte abordagem a ser realizada no agrupamento de séries temporais:

\begin{itemize}
	\item Realizar algum tipo de normalização das curvas. Sugere-se a normalização Z ou normalização max.
	\item Escolher alguma medida de dissimilaridade. Sugere-se a CID-DTW caso o conjunto de dados não seja muito grande, ou os recursos computacionais escassos. Em qualquer uma dessas duas hipóteses, sugere-se a utilização da CID-Euclidiana.
	\item Para se definir o número de grupos $k$, realizar o agrupamento do conjunto de dados já normalizados pelo algoritmo hierárquico com \emph{linkagem average} e medida de dissimilaridade escolhida. Em seguida, obter diferentes partições a partir da poda do dendrograma obtido por valores iterativos de $k$, em um intervalo no qual se espera que se encontre o valor de $k$.
	\item Em seguida, para cada uma das partições obtidas, realizar a avaliação de cada uma delas pelo índice Calinski-Harabasz, e aquela que possuir o melhor valor indicará o valor de $k$.
	\item Caso se deseje realizar um refinamento da partição obtida até o momento, uma vez definido o número de grupos $k$, pode-se realizar outras partições com diferentes escolhas de combinações de algoritmos e métricas de dissimilaridade e avaliá-las pelo índice silhouette. A partição que acarretar no melhor valor do índice é a partição final da tarefa de agrupamento.
\end{itemize}

Após a definição dos passos descritos anteriormente, obtidos por meio de testes em bases de dados rotuladas, estes foram aplicados em uma base de dados não rotulada no Capitulo ~\ref{cap:estudo_de_caso}. O estudo de caso consistiu no agrupamento dos dados de consumo de cargas residenciais, gerados por medidores inteligentes instalados em residências australianas. As curvas diárias de consumo de cada residência foram divididas em seis subconjuntos, formados cada um pela média das curvas diárias em cada uma das seis possíveis combinações entre as estações do ano; inverno, verão e transição (outono e primavera), e os tipos de dias da semana; dias de final de semana e dias de semana.

Os resultados obtidos, em todos os subconjuntos, por meio dos passos descritos anteriormente, não foram satisfatórios, pois partições altamente desbalanceadas foram obtidas. Por partições altamente desbalanceadas entende-se partições dos dados nas quais um único grupo contém quase a totalidade dos dados, ao passo que os demais grupos, denominados grupos minoritários, contêm poucas instâncias. Partições com essas características são indesejadas, pois, na maioria das vezes, não é possível identificar padrões relevantes no agrupamento dos dados. Mesmo após a eliminação das cargas contidas nos grupos minoritários, por estas terem sido, inicialmente, consideradas \emph{outliers}, a realização de novos agrupamentos acarretou, novamente, em partições altamente desbalanceadas.

Assim, decidiu-se utilizar uma técnica de avaliação de partição, que pelo menos no estudo realizado nesta dissertação, não foi visto em nenhuma das fontes bibliográficas utilizadas e mencionadas neste trabalho. Na abordagem proposta, cada uma das partições obtidas foram avaliadas por dois critérios: pela qualidade da partição, indicada pelo índice interno silhouette, e pelo balanceamento da mesma, indicado pelo desvio padrão da porcentagem de instâncias nos grupos da partição em análise. Nas partições obtidas por meio de diversas combinações de algoritmo de agrupamento e métrica de dissimilaridade, verificou-se que estes dois critérios são conflitantes, ou seja, partições que apresentavam os melhores valores de silhouette possuíam os piores valores de desvio padrão, ao passo que as partições com os melhores valores de desvio padrão possuíam os piores valores de silhouette.

Dado o caráter conflitante entre as duas características almejadas para a partição final, ficou caracterizado um problema de otimização multi-objetivo, e assim fez-se necessário encontrar as soluções, ou partições não dominadas. A partição final escolhida, foi aquela que pertence à fronteira Pareto-ótima que se encontra mais perto do ponto ótimo, ou seja do ponto $[1,0]$, no qual a partição possui silhouette igual a $1$ e desvio padrão igual a $0$. As partições encontradas para cada um dos seis subgrupos formados apresentaram alguns padrões interessantes:

\begin{itemize}
	\item Todos os subgrupos encontrados utilizaram como algoritmo de agrupamento o \emph{k-means}, apesar de outros também terem sido testados, do que se  pode inferir que o \emph{k-means} gera partições que apresentam um bom compromisso entre a qualidade da partição e o balanceamento do número de instâncias nos grupos.
	\item Os subgrupos que continham dados de curva de carga obtidas nos finais de semana foram particionados em um número de grupos significativamente maior do que os subgrupos contendo curvas obtidas nos dias de semana, o que indica que existe uma maior heterogeneidade de perfis de consumo residenciais nos finais de semana em comparação com os dias de semana.
\end{itemize}

O fato da estratégia definida a partir dos experimentos realizados em bases de dados rotuladas no Capítulo ~\ref{cap:testes_teoricos} não ter gerado um resultado satisfatório ao ser aplicada em uma base de dado não rotulada no Capítulo ~\ref{cap:estudo_de_caso} não a desmerece, já que ela deve ser vista como uma sugestão inicial para problemas de agrupamento de séries temporais, e não como uma solução definitiva. Vale destacar que algumas decisões da tarefa de agrupamento de séries temporais, como a escolha da métrica de dissimilaridade, dependem, muitas vezes, das características intrínsecas do problema em questão. No estudo realizado com as bases rotuladas, foram utilizadas séries temporais de diversas aplicações e áreas do conhecimento como sinais biomédicos, problemas de visão computacional, detecção de movimento, sinais de equipamentos elétricos, entre outros. Assim, reforça-se a sua sugestão como uma primeira alternativa para a realização do agrupamento de séries temporais, e caso resultados insatisfatórios sejam obtidos com ela, outras abordagens podem ser adotadas.

No contexto de curvas de cargas obtidas por medidores inteligentes, o estudo de caso realizado no Capítulo ~\ref{cap:estudo_de_caso} indica que o algoritmo \emph{k-means}, aliado à métrica de dissimilaridade euclidiana, fornece resultados satisfatórios. Tais escolhas estão em consonância com a literatura ~\parencite{Chicco} e são interessantes para a realização de agrupamento de grandes massas de dados, como as que serão geradas após a massificação da instalação de medidores inteligentes no sistema elétrico de potência brasileiro, pois a distância euclidiana tem custo computacional significativamente menor que suas principais concorrentes (DTW e EDR), além do algoritmo \emph{k-means} ser altamente escalável \parencite{zhao2009parallel, stoffel1999parallel, bahmani2012scalable}. 

Assim, pode-se considerar que este contribui na indicação de metodologias (escolhas), no que diz respeito a pré-processamento, algoritmo de agrupamento e métrica de dissimilaridade, dentre as várias existentes na literatura, a serem utilizadas no agrupamento de curvas de carga obtidas por medidores inteligentes.

\section{Propostas de trabalhos futuros}

A dissertação de mestrado apresentada é somente uma fração do estudo de agrupamento de séries temporais e do seu emprego em agrupamentos de curvas de cargas, assim, sugere-se o aprofundamento nos seguintes temas:

\begin{itemize}
	\item Avaliar, no contexto de agrupamento, a performance da medida de dissimilaridade TWED (\emph{Time Warp Edit Distance }) proposta em \parencite{10.1109/TPAMI.2008.76}, que em experimentos realizados em problemas de classificação, expostos em \parencite{Serra},  apresentou performance superior à distância euclidiana, DTW, EDR e suas respectivas correções pela CID.
	\item Estudar a variação no cálculo da DTW descrita em \parencite{Petitjean:2011:GAM:1890011.1890193} que faz com que seja possível a utilização dela no algoritmo \emph{k-means}.
	\item Verificar os métodos descritos em \parencite{Morse:2007:EAM:1247480.1247544} que reduzem os custos computacionais envolvidos no cálculo da DTW e EDR.
	\item Estudar as técnicas de agrupamento consensuais, que combinam diversas partições para a obtenção de uma partição final, no contexto de séries temporais ~\parencite{consenso}.
	\item Investigar a robustez da proposta de agrupamento multi-objetiva realizada nesta dissertação para o agrupamento de conjuntos de dados desbalanceados.
\end{itemize}